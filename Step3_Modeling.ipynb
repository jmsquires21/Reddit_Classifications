{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eff386fe-ecdc-479c-be03-280ab5f09d31",
   "metadata": {},
   "source": [
    "# Step 3: Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30442cae-91a1-41f0-b8d9-7597425f1f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegressionCV, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad040dec-3576-40e2-a7e7-cfdd8f25b2cc",
   "metadata": {},
   "source": [
    "### Read in step 1 data & prep for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ed00898-1570-4e0a-9880-c087b459048d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>ds_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1638348806</td>\n",
       "      <td>Ideally would love a program that goes at my o...</td>\n",
       "      <td>datascience</td>\n",
       "      <td>Can a master's in DS from WGU or like universi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1638348235</td>\n",
       "      <td>NaN</td>\n",
       "      <td>datascience</td>\n",
       "      <td>Data Scientists in Germany: What to expect as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1638344086</td>\n",
       "      <td>NaN</td>\n",
       "      <td>datascience</td>\n",
       "      <td>How's Pluralsight for Data Science?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1638341696</td>\n",
       "      <td>I am trying to implement this for car dashcam ...</td>\n",
       "      <td>datascience</td>\n",
       "      <td>vid2depth on custom video</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1638333037</td>\n",
       "      <td>\\n\\nShould I buy this offer if I now the basi...</td>\n",
       "      <td>datascience</td>\n",
       "      <td>DataCamp CyberMonday Offer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   created_utc                                           selftext  \\\n",
       "0   1638348806  Ideally would love a program that goes at my o...   \n",
       "1   1638348235                                                NaN   \n",
       "2   1638344086                                                NaN   \n",
       "3   1638341696  I am trying to implement this for car dashcam ...   \n",
       "4   1638333037   \\n\\nShould I buy this offer if I now the basi...   \n",
       "\n",
       "     subreddit                                              title  ds_ind  \n",
       "0  datascience  Can a master's in DS from WGU or like universi...       1  \n",
       "1  datascience  Data Scientists in Germany: What to expect as ...       1  \n",
       "2  datascience                How's Pluralsight for Data Science?       1  \n",
       "3  datascience                          vid2depth on custom video       1  \n",
       "4  datascience                         DataCamp CyberMonday Offer       1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the data we're looking at from part 1\n",
    "both_posts_imp = pd.read_csv('../data/both_posts_step1_xl.csv')\n",
    "both_posts_imp.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "both_posts_imp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0180750-a87f-4eb1-b503-032c421789fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19996, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "both_posts_imp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bc4a38a-794c-4eb2-959c-446333d8cdc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_utc       0\n",
       "selftext       3520\n",
       "subreddit         0\n",
       "title             0\n",
       "ds_ind            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "both_posts_imp.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "496b82a6-4f36-41a8-a938-a3f8ba856422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datascience    10000\n",
       "analytics       9996\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "both_posts_imp['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e04362d-83b7-4dcc-9d8b-58ce352224f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14711</th>\n",
       "      <td>analytics</td>\n",
       "      <td>\"A theory of everything\":Mathematics shown to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14246</th>\n",
       "      <td>analytics</td>\n",
       "      <td>\"Avg Page Load time(sec.)\" metric formula in G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10064</th>\n",
       "      <td>analytics</td>\n",
       "      <td>\"Betting in the economic perspetives\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15974</th>\n",
       "      <td>analytics</td>\n",
       "      <td>\"Data Analyst\" without analytics skills. Looki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7247</th>\n",
       "      <td>datascience</td>\n",
       "      <td>\"Data scientists will be extinct in 10 years\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>datascience</td>\n",
       "      <td>üòçStraight out of science fiction: Separate cli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14593</th>\n",
       "      <td>analytics</td>\n",
       "      <td>üòì Monthly/Quarterly Reporting Dreadful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8044</th>\n",
       "      <td>datascience</td>\n",
       "      <td>üöÄ How to Crack the Facebook Data Scientist Int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>datascience</td>\n",
       "      <td>ü§ØüñºÔ∏èRemove any object or person in an image eas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>datascience</td>\n",
       "      <td>üß© Configuring PostgreSQL as Auth0 Custom Database</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19996 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         subreddit                                              title\n",
       "14711    analytics  \"A theory of everything\":Mathematics shown to ...\n",
       "14246    analytics  \"Avg Page Load time(sec.)\" metric formula in G...\n",
       "10064    analytics              \"Betting in the economic perspetives\"\n",
       "15974    analytics  \"Data Analyst\" without analytics skills. Looki...\n",
       "7247   datascience  \"Data scientists will be extinct in 10 years\" ...\n",
       "...            ...                                                ...\n",
       "1283   datascience  üòçStraight out of science fiction: Separate cli...\n",
       "14593    analytics             üòì Monthly/Quarterly Reporting Dreadful\n",
       "8044   datascience  üöÄ How to Crack the Facebook Data Scientist Int...\n",
       "2128   datascience  ü§ØüñºÔ∏èRemove any object or person in an image eas...\n",
       "2066   datascience  üß© Configuring PostgreSQL as Auth0 Custom Database\n",
       "\n",
       "[19996 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspect target (subreddit) and feature(title)\n",
    "viewing_purposes=both_posts_imp.filter([\"subreddit\",\"title\"])\n",
    "viewing_purposes.sort_values('title')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622d344a-042b-47f0-a92e-d40e0759ce8e",
   "metadata": {},
   "source": [
    "### Setup Target X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8831b59d-2189-4005-98db-26a4fb214143",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up train/test with the default .75/.25 train/test size\n",
    "\n",
    "X=both_posts_imp['title']\n",
    "y=both_posts_imp['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599fb01b-d831-4427-a0da-dfd515690df5",
   "metadata": {},
   "source": [
    "### Baseline Accuracy for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a393a19-b8f5-40ac-acb9-6d9408175d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datascience    0.5001\n",
       "analytics      0.4999\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "both_posts_imp['subreddit'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a5a47a-b92a-4050-9768-e5f361b68394",
   "metadata": {},
   "source": [
    "### Create Stopwords Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acca6e09-3705-4e09-ba02-e2d2398606e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_custom1 = nltk.corpus.stopwords.words('english')\n",
    "stopwords_custom1.append(\"data science\")\n",
    "stopwords_custom1.append(\"analytics\")\n",
    "stopwords_custom1.append(\"science\")\n",
    "stopwords_custom1.append(\"scientists\")\n",
    "stopwords_custom1.append(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61d7f12d-2d18-485d-a3e1-87238b4567a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_custom2 = nltk.corpus.stopwords.words('english')\n",
    "stopwords_custom2.append(\"data science\")\n",
    "stopwords_custom2.append(\"data\")\n",
    "stopwords_custom2.append(\"analytics\")\n",
    "stopwords_custom2.append(\"science\")\n",
    "stopwords_custom2.append(\"scientists\")\n",
    "stopwords_custom2.append(\"get\")\n",
    "stopwords_custom2.append(\"amp\")\n",
    "stopwords_custom2.append(\"would\")\n",
    "stopwords_custom2.append(\"using\")\n",
    "stopwords_custom2.append(\"use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cdbe05-1b10-4bce-b100-d09913586455",
   "metadata": {},
   "source": [
    "## Initial Step 1 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed25af3-b478-4a8a-a48d-d5b38deb11fd",
   "metadata": {},
   "source": [
    "### CountVectorizer Passes\n",
    "* Explore how different models interact with CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0bee9f9-0dc6-4840-91e8-17fe391d7bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8723077948923118, 0.8013602720544108)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count Vectorizer LogReg\n",
    "pipe = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('lr', LogisticRegressionCV(solver='liblinear'))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_train, y_train), pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c540d22-e34e-4d0d-b847-d307dff723cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8501033540041342, 0.7871574314862972)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count Vectorizer Naive Bayes\n",
    "\n",
    "pipe_nb = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe_nb.fit(X_train, y_train)\n",
    "pipe_nb.score(X_train, y_train), pipe_nb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ffb7b0e-0be5-4321-b288-7e2c2d80148c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9935987197439488, 0.7643528705741148)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count Vectorizer Decision Trees\n",
    "pipe = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('dt', DecisionTreeClassifier())\n",
    "])\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_train, y_train), pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2918de8-2fe0-4c91-8fab-d98d25fd7a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9935320397412816, 0.7851570314062812)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count Vectorizer Bagging Classifier\n",
    "pipe_bc = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('bc', BaggingClassifier(random_state=42, n_estimators=100))\n",
    "])\n",
    "pipe_bc.fit(X_train, y_train)\n",
    "pipe_bc.score(X_train, y_train), pipe_bc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca07b256-8eb3-43ab-87a2-24ca0431a1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.991998399679936, 0.7953590718143628)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count Vectorizer Random Forests\n",
    "\n",
    "pipe_rf_tf = Pipeline([\n",
    "    ('cv', CountVectorizer(min_df=2)),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "pipe_rf_tf.fit(X_train, y_train)\n",
    "pipe_rf_tf.score(X_train, y_train), pipe_rf_tf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1efeae3-12b6-40d7-bbaa-a51ba18321d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8255651130226045, 0.7821564312862572)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count Vectorizer AdaBoost Classifier\n",
    "\n",
    "pipe_abc1 = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('abc', AdaBoostClassifier(random_state=42, n_estimators=500))\n",
    "])\n",
    "pipe_abc1.fit(X_train, y_train)\n",
    "pipe_abc1.score(X_train, y_train), pipe_abc1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9315d56f-1c02-4777-af1f-ac8c627de0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8271654330866173, 0.7931586317263453)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count Vectorizer Gradient Boost Classifier\n",
    "\n",
    "pipe_gb1 = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('gb', GradientBoostingClassifier(random_state=42, n_estimators=500))\n",
    "])\n",
    "pipe_gb1.fit(X_train, y_train)\n",
    "pipe_gb1.score(X_train, y_train), pipe_gb1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3738293-a9ad-417e-b479-33a940bccedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9399213175968527, 0.7669533906781356)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count Vectorizer SVM\n",
    "\n",
    "pipe_svm1 = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('svm', LinearSVC(max_iter=100))\n",
    "])\n",
    "pipe_svm1.fit(X_train, y_train)\n",
    "pipe_svm1.score(X_train, y_train), pipe_svm1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaf5a54-e5f8-42e1-b636-77e49d4e3ef7",
   "metadata": {},
   "source": [
    "### TF-IDF Passes\n",
    "* Explore how different models interact with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aef21d4f-fcb4-4671-ae94-1a793039f151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8459691938387678, 0.8025605121024205)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf Log Regression\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('tf', TfidfVectorizer()),\n",
    "    ('lr', LogisticRegressionCV(solver = 'liblinear'))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_train, y_train), pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe0468d1-34e9-4598-9163-653a62cd5a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8514369540574782, 0.7699539907981596)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf Naive Bayes\n",
    "\n",
    "pipe_nb = Pipeline([\n",
    "    ('tf', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe_nb.fit(X_train, y_train)\n",
    "pipe_nb.score(X_train, y_train), pipe_nb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34322664-0533-4b45-aee0-9d33cb1131ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6191905047676202, 0.5471094218843768)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tfidf KNN\n",
    "pipe = Pipeline([\n",
    "    ('tf', TfidfVectorizer()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_train, y_train), pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa954036-d100-4355-b3d5-cb7ab1f2de47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9935987197439488, 0.7473494698939788)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf Decision Trees\n",
    "pipe = Pipeline([\n",
    "    ('tf', TfidfVectorizer()),\n",
    "    ('dt', DecisionTreeClassifier())\n",
    "])\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_train, y_train), pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "724a03e8-90a1-4f0e-a3ea-637c738e8266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9935320397412816, 0.7869573914782957)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf Bagging Classifier\n",
    "pipe_bc = Pipeline([\n",
    "    ('tf', TfidfVectorizer()),\n",
    "    ('bc', BaggingClassifier(random_state=42, n_estimators=100))\n",
    "])\n",
    "pipe_bc.fit(X_train, y_train)\n",
    "pipe_bc.score(X_train, y_train), pipe_bc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2911fef0-9f5d-4f73-9def-10b7b843de8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9935987197439488, 0.7997599519903981)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf Random Forests\n",
    "pipe_rf1 = Pipeline([\n",
    "    ('tf', TfidfVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "pipe_rf1.fit(X_train, y_train)\n",
    "pipe_rf1.score(X_train, y_train), pipe_rf1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d6ef52e-3280-4811-ae72-05c984b87ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8351670334066813, 0.7765553110622124)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf AdaBoost Classifier\n",
    "\n",
    "pipe_abc1 = Pipeline([\n",
    "    ('tf', TfidfVectorizer()),\n",
    "    ('abc', AdaBoostClassifier(random_state=42, n_estimators=500))\n",
    "])\n",
    "pipe_abc1.fit(X_train, y_train)\n",
    "pipe_abc1.score(X_train, y_train), pipe_abc1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36086f35-80f2-489a-9a26-e2411f1df723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8365673134626925, 0.7889577915583117)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf Gradient Boost Classifier\n",
    "\n",
    "pipe_gb1 = Pipeline([\n",
    "    ('tf', TfidfVectorizer()),\n",
    "    ('gb', GradientBoostingClassifier(random_state=42, n_estimators=500))\n",
    "])\n",
    "pipe_gb1.fit(X_train, y_train)\n",
    "pipe_gb1.score(X_train, y_train), pipe_gb1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84103d24-a68e-4d79-824a-d56f513d30cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9235847169433887, 0.7865573114622925)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf SVM\n",
    "\n",
    "pipe_svm1 = Pipeline([\n",
    "    ('tf', TfidfVectorizer()),\n",
    "    ('svm', LinearSVC(max_iter=100))\n",
    "])\n",
    "pipe_svm1.fit(X_train, y_train)\n",
    "pipe_svm1.score(X_train, y_train), pipe_svm1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e841f290-cbcb-4ec4-bc30-492308282745",
   "metadata": {},
   "source": [
    "### Preliminary Results:\n",
    "* Based on the runs above, we will choose to experiment with different hyperparameters via GridSearch to tune the model.\n",
    "* Logistic Regression, Random Forests, and Gradient Boosting had the highest testing accuracy scores so we will focus primarily on these. In some cases, TF-IDF did slightly better than CountVectorizer so we will focus on these models as well.\n",
    "* These initial passes show that several models have high variance and overfitting, so we will explore different hyperparameters to help with the overfitting as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffcbafc-7c85-442b-8b65-585130e320a2",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2 Models: \n",
    "* Improve model performance with hyperparameters via GridSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb925c79-741b-4d11-9bbf-af004ff4a398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr__Cs': [0.45], 'lr__max_iter': 100, 'lr__penalty': 'l2', 'lr__random_state': 42, 'tf__max_features': None, 'tf__min_df': 1, 'tf__ngram_range': (1, 2), 'tf__stop_words': 'english'}\n",
      "Cross Val: 0.8021606090919194\n",
      "Training Accuracy: 0.8898446355937855\n",
      "Testing Accuracy: 0.8063612722544509\n"
     ]
    }
   ],
   "source": [
    "# TFID Vectorizer X Logistic Regression\n",
    "\n",
    "pipe_log_reg1c = Pipeline([\n",
    "    ('tf', TfidfVectorizer()),\n",
    "    ('lr', LogisticRegressionCV(solver='saga'))\n",
    "])\n",
    "pipe_log_reg1c.fit(X_train, y_train)\n",
    "pipe_log_reg1c.score(X_train, y_train), pipe_log_reg1c.score(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "logreg_params = {\n",
    "    'tf__max_features': [None, 1000],\n",
    "    'tf__min_df': [1,2],\n",
    "    'tf__stop_words': [None, 'english', stopwords_custom1, stopwords_custom2],\n",
    "    'tf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'lr__Cs': [[.1],[.2],[.3],[.35],[.4],[.45],[.5]],\n",
    "    'lr__penalty':['l1', 'l2', 'elasticnet'],\n",
    "    'lr__max_iter':range(100,300,100),\n",
    "    'lr__random_state':[42]\n",
    "}\n",
    "\n",
    "\n",
    "pipe_log_reg2c = GridSearchCV(pipe_log_reg1c, # What model do we want to fit?\n",
    "                                logreg_params, # What is the dictionary of hyperparameters\n",
    "                                cv = 5, # what number of folds in CV will we use?\n",
    "                                #verbose = 1,\n",
    "                                n_jobs= -1 )\n",
    "\n",
    "pipe_log_reg2c.fit(X_train, y_train)\n",
    "print(pipe_log_reg2c.best_params_)\n",
    "print(f'Cross Val: {pipe_log_reg2c.best_score_}')\n",
    "print(f'Training Accuracy: {pipe_log_reg2c.score(X_train, y_train)}')\n",
    "print(f'Testing Accuracy: {pipe_log_reg2c.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "60cd11f4-c6b1-4e0f-a1a7-e09be661c5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cv__max_features': None, 'cv__min_df': 1, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english', 'lr__Cs': [0.2], 'lr__max_iter': 100, 'lr__penalty': 'l2', 'lr__random_state': 42}\n",
      "Cross Val: 0.8008936978992998\n",
      "Training Accuracy: 0.909381876375275\n",
      "Testing Accuracy: 0.8069613922784556\n"
     ]
    }
   ],
   "source": [
    "# Count Vectorizer X Logistic Regression\n",
    "\n",
    "pipe_log_reg1d = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('lr', LogisticRegressionCV(solver='saga'))\n",
    "])\n",
    "pipe_log_reg1d.fit(X_train, y_train)\n",
    "pipe_log_reg1d.score(X_train, y_train), pipe_log_reg1d.score(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "logreg_params = {\n",
    "    'cv__max_features': [None, 1000],\n",
    "    'cv__min_df': [1,2],\n",
    "    'cv__stop_words': [None, 'english', stopwords_custom1, stopwords_custom2],\n",
    "    'cv__ngram_range': [(1, 1), (1, 2)],\n",
    "    'lr__Cs': [[.1],[.2],[.3],[.35],[.4],[.45],[.5]],\n",
    "    'lr__penalty':['l1', 'l2', 'elasticnet'],\n",
    "    'lr__max_iter':range(100,300,100),\n",
    "    'lr__random_state':[42]\n",
    "}\n",
    "\n",
    "\n",
    "pipe_log_reg2d = GridSearchCV(pipe_log_reg1d, # What model do we want to fit?\n",
    "                                logreg_params, # What is the dictionary of hyperparameters\n",
    "                                cv = 5, # what number of folds in CV will we use?\n",
    "                                verbose = 1,\n",
    "                                n_jobs= -1 )\n",
    "\n",
    "pipe_log_reg2d.fit(X_train, y_train)\n",
    "print(pipe_log_reg2d.best_params_)\n",
    "print(f'Cross Val: {pipe_log_reg2d.best_score_}')\n",
    "print(f'Training Accuracy: {pipe_log_reg2d.score(X_train, y_train)}')\n",
    "print(f'Testing Accuracy: {pipe_log_reg2d.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32ed6a2b-2fc6-411e-a1ad-312f46947336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "{'tf__max_features': None, 'tf__min_df': 1, 'tf__ngram_range': (1, 2), 'tf__stop_words': 'english'}\n",
      "Cross Val: 0.7860904301433811\n",
      "Training Accuracy: 0.9381876375275054\n",
      "Testing Accuracy: 0.7935587117423485\n"
     ]
    }
   ],
   "source": [
    "# TFID Vectorizer X Naive Bayes\n",
    "\n",
    "pipe_nb1 = Pipeline([\n",
    "    ('tf', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "pipe_nb1.fit(X_train, y_train)\n",
    "pipe_nb1.score(X_train, y_train), pipe_nb1.score(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "nb_params = {\n",
    "    'tf__max_features': [None, 1000,2000],\n",
    "    'tf__min_df': [1,2],\n",
    "    'tf__stop_words': [None, 'english', stopwords_custom1, stopwords_custom2],\n",
    "    'tf__ngram_range': [(1, 1), (1, 2)]\n",
    "}\n",
    "\n",
    "\n",
    "pipe_nb2 = GridSearchCV(pipe_nb1, # What model do we want to fit?\n",
    "                                nb_params, # What is the dictionary of hyperparameters\n",
    "                                cv = 5, # what number of folds in CV will we use?\n",
    "                                verbose = 1,\n",
    "                                n_jobs= -1 )\n",
    "\n",
    "pipe_nb2.fit(X_train, y_train)\n",
    "print(pipe_nb2.best_params_)\n",
    "print(f'Cross Val: {pipe_nb2.best_score_}')\n",
    "print(f'Training Accuracy: {pipe_nb2.score(X_train, y_train)}')\n",
    "print(f'Testing Accuracy: {pipe_nb2.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd9ccf0c-80d3-4cf9-b19a-c9d0fd5d4fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 480 candidates, totalling 2400 fits\n",
      "{'knn__metric': 'euclidean', 'knn__n_neighbors': 41, 'tf__max_features': None, 'tf__min_df': 1, 'tf__ngram_range': (1, 2), 'tf__stop_words': 'english'}\n",
      "Cross Val: 0.7500170056685562\n",
      "Training Accuracy: 0.7596185903847437\n",
      "Testing Accuracy: 0.7451490298059612\n"
     ]
    }
   ],
   "source": [
    "# TFID Vectorizer X KNN\n",
    "\n",
    "pipe_knn1 = Pipeline([\n",
    "    ('tf', TfidfVectorizer()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "pipe_knn1.fit(X_train, y_train)\n",
    "pipe_knn1.score(X_train, y_train), pipe_knn1.score(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "knn_params = {\n",
    "    'tf__max_features': [None, 1000,2000],\n",
    "    'tf__min_df': [1,2],\n",
    "    'tf__stop_words': [None, 'english', stopwords_custom1, stopwords_custom2],\n",
    "    'tf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'knn__n_neighbors': range(1, 51, 10),\n",
    "    'knn__metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "\n",
    "pipe_knn2 = GridSearchCV(pipe_knn1, # What model do we want to fit?\n",
    "                                knn_params, # What is the dictionary of hyperparameters\n",
    "                                cv = 5, # what number of folds in CV will we use?\n",
    "                                verbose = 1,\n",
    "                                n_jobs= -1 )\n",
    "\n",
    "pipe_knn2.fit(X_train, y_train)\n",
    "print(pipe_knn2.best_params_)\n",
    "print(f'Cross Val: {pipe_knn2.best_score_}')\n",
    "print(f'Training Accuracy: {pipe_knn2.score(X_train, y_train)}')\n",
    "print(f'Testing Accuracy: {pipe_knn2.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "30b51f43-17dc-4e51-ab28-227a4d1b3f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7200 candidates, totalling 36000 fits\n",
      "{'dt__max_depth': None, 'dt__min_samples_leaf': 6, 'dt__min_samples_split': 20, 'tf__max_features': 2000, 'tf__min_df': 1, 'tf__ngram_range': (1, 1), 'tf__stop_words': 'english'}\n",
      "Cross Val: 0.7589519839946648\n",
      "Training Accuracy: 0.8529705941188238\n",
      "Testing Accuracy: 0.75375075015003\n"
     ]
    }
   ],
   "source": [
    "# TFID Vectorizer X Decision Trees\n",
    "\n",
    "pipe_dt1 = Pipeline([\n",
    "    ('tf', TfidfVectorizer()),\n",
    "    ('dt', DecisionTreeClassifier())\n",
    "])\n",
    "pipe_dt1.fit(X_train, y_train)\n",
    "pipe_dt1.score(X_train, y_train), pipe_dt1.score(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "dt_params = {\n",
    "    'tf__max_features': [None, 1000,2000],\n",
    "    'tf__min_df': [1,2],\n",
    "    'tf__stop_words': [None, 'english', stopwords_custom1, stopwords_custom2],\n",
    "    'tf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'dt__max_depth': [None, 2, 3, 5, 7],\n",
    "    'dt__min_samples_split': [2, 5, 10, 15, 20],\n",
    "    'dt__min_samples_leaf': range(1, 7),\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "pipe_dt2 = GridSearchCV(pipe_dt1, # What model do we want to fit?\n",
    "                                dt_params, # What is the dictionary of hyperparameters\n",
    "                                cv = 5, # what number of folds in CV will we use?\n",
    "                                verbose = 1,\n",
    "                                n_jobs= -1 )\n",
    "\n",
    "pipe_dt2.fit(X_train, y_train)\n",
    "print(pipe_dt2.best_params_)\n",
    "print(f'Cross Val: {pipe_dt2.best_score_}')\n",
    "print(f'Training Accuracy: {pipe_dt2.score(X_train, y_train)}')\n",
    "print(f'Testing Accuracy: {pipe_dt2.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb9aa14a-f6ab-4419-9f8f-8f0f8db1fb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1440 candidates, totalling 7200 fits\n",
      "{'rf__max_depth': None, 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 2, 'rf__n_estimators': 400, 'rf__random_state': 42}\n",
      "Cross Val: 0.7843571857285763\n",
      "Training Accuracy: 0.8822431152897247\n",
      "Testing Accuracy: 0.7885577115423085\n"
     ]
    }
   ],
   "source": [
    "# TFID Vectorizer X Random Forests\n",
    "\n",
    "#Trying this with more narrowed down params (V1)\n",
    "\n",
    "\n",
    "pipe_rf1 = Pipeline([\n",
    "    ('tf', TfidfVectorizer(max_features=1000, ngram_range=(1,2),stop_words='english', min_df=1)),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "pipe_rf1.fit(X_train, y_train)\n",
    "pipe_rf1.score(X_train, y_train), pipe_rf1.score(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "rf_params = {\n",
    "   # 'tf__max_features': [None, 1000,2000],\n",
    "   # 'tf__min_df': [0,1,2],\n",
    "   # 'tf__stop_words': [None, 'english', stopwords_custom1, stopwords_custom2],\n",
    "   # 'tf__ngram_range': [(1, 1), (1, 2)],\n",
    "    \n",
    "    'rf__max_depth': [None, 2, 3, 5, 7],\n",
    "    'rf__min_samples_split': [2, 5, 10, 15, 20],\n",
    "    'rf__min_samples_leaf': range(1, 7),\n",
    "    'rf__n_estimators': range(100, 500, 50),\n",
    "    'rf__max_depth': [None, 1, 2, 3, 4, 5],\n",
    "    'rf__random_state':[42]\n",
    "}\n",
    "\n",
    "\n",
    "pipe_rf2 = GridSearchCV(pipe_rf1, # What model do we want to fit?\n",
    "                                rf_params, # What is the dictionary of hyperparameters\n",
    "                                cv = 5, # what number of folds in CV will we use?\n",
    "                                verbose = 1,\n",
    "                                n_jobs= -1 )\n",
    "\n",
    "pipe_rf2.fit(X_train, y_train)\n",
    "print(pipe_rf2.best_params_)\n",
    "print(f'Cross Val: {pipe_rf2.best_score_}')\n",
    "print(f'Training Accuracy: {pipe_rf2.score(X_train, y_train)}')\n",
    "print(f'Testing Accuracy: {pipe_rf2.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6bd9795-7514-4a5f-8d4a-d77e6334ad06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 384 candidates, totalling 1920 fits\n",
      "{'gb__n_estimators': 400, 'gb__random_state': 42, 'tf__max_features': None, 'tf__min_df': 2, 'tf__ngram_range': (1, 1), 'tf__stop_words': 'english'}\n",
      "Cross Val: 0.7860240969211959\n",
      "Training Accuracy: 0.8259651930386077\n",
      "Testing Accuracy: 0.7885577115423085\n"
     ]
    }
   ],
   "source": [
    "# TFID Vectorizer X Gradient Boosting \n",
    "\n",
    "\n",
    "pipe_gb1 = Pipeline([\n",
    "    ('tf', TfidfVectorizer()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "])\n",
    "pipe_gb1.fit(X_train, y_train)\n",
    "pipe_gb1.score(X_train, y_train), pipe_gb1.score(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "gb_params = {\n",
    "    'tf__max_features': [None, 1000,2000],\n",
    "    'tf__min_df': [1,2],\n",
    "    'tf__stop_words': [None, 'english', stopwords_custom1, stopwords_custom2],\n",
    "    'tf__ngram_range': [(1, 1), (1, 2)],\n",
    "    \n",
    "    'gb__n_estimators': range(100, 500, 50),\n",
    "    'gb__random_state':[42]\n",
    "}\n",
    "\n",
    "\n",
    "pipe_gb2 = GridSearchCV(pipe_gb1, # What model do we want to fit?\n",
    "                                gb_params, # What is the dictionary of hyperparameters\n",
    "                                cv = 5, # what number of folds in CV will we use?\n",
    "                                verbose = 1,\n",
    "                                n_jobs= -1 )\n",
    "\n",
    "pipe_gb2.fit(X_train, y_train)\n",
    "print(pipe_gb2.best_params_)\n",
    "print(f'Cross Val: {pipe_gb2.best_score_}')\n",
    "print(f'Training Accuracy: {pipe_gb2.score(X_train, y_train)}')\n",
    "print(f'Testing Accuracy: {pipe_gb2.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e0d32cb-0142-4ed0-a314-f7199206fdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 256 candidates, totalling 1280 fits\n",
      "{'cv__max_features': 1000, 'cv__min_df': 1, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english', 'gb__n_estimators': 450, 'gb__random_state': 42}\n",
      "Cross Val: 0.7898252973213292\n",
      "Training Accuracy: 0.8166966726678669\n",
      "Testing Accuracy: 0.7945589117823565\n"
     ]
    }
   ],
   "source": [
    "# Count Vectorizer X Gradient Boosting \n",
    "\n",
    "\n",
    "pipe_gb1b = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "])\n",
    "pipe_gb1b.fit(X_train, y_train)\n",
    "pipe_gb1b.score(X_train, y_train), pipe_gb1b.score(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "gb_b_params = {\n",
    "    'cv__max_features': [None, 1000],\n",
    "    'cv__min_df': [1,2],\n",
    "    'cv__stop_words': [None, 'english', stopwords_custom1, stopwords_custom2],\n",
    "    'cv__ngram_range': [(1, 1), (1, 2)],\n",
    "    \n",
    "    'gb__n_estimators': range(100, 500, 50),\n",
    "    'gb__random_state':[42]\n",
    "}\n",
    "\n",
    "\n",
    "pipe_gb2b = GridSearchCV(pipe_gb1b, # What model do we want to fit?\n",
    "                               gb_b_params, # What is the dictionary of hyperparameters\n",
    "                                cv = 5, # what number of folds in CV will we use?\n",
    "                                verbose = 1,\n",
    "                                n_jobs= -1 )\n",
    "\n",
    "pipe_gb2b.fit(X_train, y_train)\n",
    "print(pipe_gb2b.best_params_)\n",
    "print(f'Cross Val: {pipe_gb2b.best_score_}')\n",
    "print(f'Training Accuracy: {pipe_gb2b.score(X_train, y_train)}')\n",
    "print(f'Testing Accuracy: {pipe_gb2b.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccc4163-557f-44c7-a3f3-68e7ce878668",
   "metadata": {},
   "source": [
    "# Results:\n",
    "* The model that performed the best was the Logistic Regression with a Count Vectorizer, with a testing accuracy of 80.7%. The cross-val score was 80.9%. The second logistic regression with TF-IDF was very close, with a test accuracy of 80.6%.\n",
    "* In our dataset, Data Science had a baseline accuracy of ~50%, thus we see that our model did better than the baseline (81% vs 50%). \n",
    "* The next model that did the best was Gradient Boost with Count Vectorizer at 79.5%. \n",
    "* Most of the remaining models performed between 75%-79%. \n",
    "* Based on these results, we can see that the two subreddits of Data Science and Analytics have a lot of overlap, with many terms in common. This may be the reason that all of our models hit a ceiling with the best accuracy at 81%.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
